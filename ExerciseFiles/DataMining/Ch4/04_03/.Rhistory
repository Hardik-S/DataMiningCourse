pacman::p_load("class")  # class has knn function
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/Desktop/GitHub/DataMiningCourse/ExerciseFiles/Datamining/Ch4/04_03/ccdefault.csv", header = T)
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/Datamining/Ch4/04_03/ccdefault.csv", header = T)
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/DataMining/Ch4/04_03/ccdefault.csv", header = T)
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/DataMining/Ch04/04_03/ccdefault.csv", header = T)
# Read CSV
states <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/DataMining/Ch3/03_03/ClusterData.csv", header = T)
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/DataMining/Ch4/04_03/ccdefault.csv", header = T)
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/DataMining/Ch4/04_03/ccdefault.xls", header = T)
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/DataMining/Ch4/04_03/ccdefault.xls", header = T)
# Read CSV
#C:\Users\hardi\Documents\GitHub\DataMiningCourse\ExerciseFiles\DataMining\Ch4\04_03
df <- read.csv("~/GitHub/DataMiningCourse/ExerciseFiles/DataMining/Ch4/04_03/ccdefault.csv", header = T)
colnames(df)
head(df)  # Show first six cases
# Define function for normalizing data
normalize <- function(x) {
norm <- ((x - min(x))/(max(x) - min(x)))
return (norm)
}
# Apply function to data frame (but not index or outcome)
dfn <- as.data.frame(lapply(df[, 2:24], normalize))
# dfn (data frame normalized)
head(dfn)
# Put outcome variable back on and rename
dfn <- cbind(dfn, df[, 25])
names(dfn)[24] <- "DEFAULT"
# Check data
colnames(dfn)
head(dfn)
# Split data into training set (2/3) and testing set (1/3)
set.seed(2786)  # Random seed
dfn.split <- sample(2, nrow(dfn),
replace = TRUE,
prob = c(4/5, 1/5))
# Create training and testing datasets without outcome
# labels. Use just the first 23 variables.
dfn.train <- dfn[dfn.split == 1, 1:23]
dfn.test  <- dfn[dfn.split == 2, 1:23]
# Create outcome labels
dfn.train.labels <- dfn[dfn.split == 1, 24]
dfn.test.labels  <- dfn[dfn.split == 2, 24]
# Build classifier for test data.
# k = number of neighbors to compare; odd n avoids ties.
# Try with several values of k and check accuracy on
# following table.
dfn.pred <- knn(train = dfn.train,
test = dfn.test,
cl = dfn.train.labels,  # true class
k = 9)                  # n neighbors
# Compare predicted outcome to observed outcome
table(dfn.pred, dfn.test.labels)
# Build classifier for test data.
# k = number of neighbors to compare; odd n avoids ties.
# Try with several values of k and check accuracy on
# following table.
dfn.pred <- knn(train = dfn.train,      # save to prediction
test = dfn.test,
cl = dfn.train.labels,  # true class
k = 21)                  # n neighbors
# Compare predicted outcome to observed outcome
table(dfn.pred, dfn.test.labels)
# Build classifier for test data.
# k = number of neighbors to compare; odd n avoids ties.
# Try with several values of k and check accuracy on
# following table.
dfn.pred <- knn(train = dfn.train,      # save to prediction
test = dfn.test,
cl = dfn.train.labels,  # true class
k = 13)                  # n neighbors
# Compare predicted outcome to observed outcome
table(dfn.pred, dfn.test.labels)
